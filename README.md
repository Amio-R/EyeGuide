# EyeGuide

## Motivation

Indoor navigation is an obstacle for the visually impaired, as most infrastructures fail to provide them an accessible solution to find their destinations. These individuals are currently hindered and left to struggle in these unfortunate situations. Proposed solutions are often reliant on external hardware, involving peripherals like sensors on walls that are detected through an application, which entails a costly integration procedure for most existing infrastructures.

## Project Objective

The final year Capstone project provides a solution that aims to use onboard phone sensors, external distance sensors and simplistic UI to generate and guide users through a mapped interior. A building will be mapped using coordinates from Google maps and then rooms(destinations) on each floor will have to be inserted using the mobile application. Visually impaired users will be able to navigate the app through Android's TalkBack functionality. Futhermore, this project requires hardware components consisting of ToF Lidar sensor and Arduino Nano for collision detection. The anticipated design will excel as a low-cost, user-friendly application that assists in interior navigation while making it more accessible and catering to the visually impaired community.
